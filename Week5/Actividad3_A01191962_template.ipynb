{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1dc28d-4e92-4571-8622-d9bac54ca7e8",
   "metadata": {},
   "source": [
    "<img src=\"https://global.utsa.edu/tec-partnership/images/logos/logotipo-horizontal-azul-transparente.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4f6e1-f302-4b51-9c25-81b4c3c5408b",
   "metadata": {},
   "source": [
    "## **Actividad 3 | Aprendizaje supervisado y no supervisado**\n",
    "### **Análisis de grandes volúmenes de datos (Gpo 10)**\n",
    "#### Tecnológico de Monterrey\n",
    "---\n",
    "*   NOMBRE: Paulina Escalante Campbell\n",
    "*   MATRÍCULA: A01191962\n",
    "---\n",
    "### **Objetivo**\n",
    "\n",
    "Aplicar algoritmos de aprendizaje supervisado y no supervisado mediante PySpark para la resolución de problemas en análisis de datos, fomentando el desarrollo de habilidades prácticas en el manejo y procesamiento eficiente de grandes conjuntos de datos.\n",
    "\n",
    "### **1. Introducción**\n",
    "\n",
    "Introducción teórica. Se deberá de documentar de forma breve los conceptos de aprendizaje supervisado, no supervisado, así como los algoritmos más representativos que se identifican en la literatura de cada tipo de aprendizaje, además de identificar aquellos que están disponible a través de PySpark.\n",
    "Proporciona una introducción clara y detallada, alineada completamente con el objetivo de la actividad. Incluye conceptos teóricos relevantes y bien explicados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd792de0-251b-49ad-8da2-6e400013c66a",
   "metadata": {},
   "source": [
    "#### **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "815ab9f0-6b99-478d-8037-03433f3584f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, sum, when, split, col, lit, max, min, expr\n",
    "from pyspark.sql.functions import to_date, var_samp, variance, var_pop, month, to_timestamp, dayofweek\n",
    "from pyspark.sql.types import NumericType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, round, concat_ws, desc, when, concat\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a54271-e1dc-4cc9-8c31-814bc73a46ba",
   "metadata": {},
   "source": [
    "### **2. Selección de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42191c-b31e-4dd4-ad7a-20070b5c2026",
   "metadata": {},
   "source": [
    "Para esta actividad, se propone recolectar una muestra de dimensión contenida (para evitar que los tiempos de procesamiento sean altos) a partir de la base de datos que estás trabajando en tu proyecto. Para ello y tomando como base la actividad previa en la cual has implementado códigos que permiten obtener particiones de la base de datos global D que cumplen con los criterios de las variables de caracterización identificadas, se propone que recuperes un número limitado de instancias de cada partición (aplicando la técnica de muestreo que propusiste en el Módulo 3, Proyecto Base de datos de Big Data, paso 4), lo que te permitirá construir una muestra M a partir de la unión de las instancias que se recuperan de este proceso.\n",
    "\n",
    "Selección de los datos: código, con documentación, donde se implementan las etapas para la construcción de muestra inicial M (punto 2)\n",
    "\n",
    "Implementa y documenta de manera excelente el código para la selección de datos, justificando cada paso con claridad y mostrando una comprensión profunda del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4e9097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Maestria_evidencia1\") \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"16g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Mejores tablas\n",
    "#spark, comentando el comando del environment para reducir el ruido del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cbbad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env-pyspark/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "25/05/21 21:46:03 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(event_time=datetime.date(2019, 11, 17), event_type='view', product_id=5300440, category_id=2053013563173241677, brand='vitek', price=17.76, user_id=513341639, user_session='d9544029-2739-4d16-9cac-79650460d9f0', event_time_ts=datetime.datetime(2019, 11, 17, 5, 35, 32), parent_category='None', subcategory='None', price_bucket='low', day_of_week=1, is_weekend=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "####\n",
    "#### La base de datos ha sido limpiada y modificada a este punto\n",
    "#### \n",
    "####\n",
    "file_path = \"/Users/pauescalante/Documents/Maestria/Trimestre 7/BigData/big-data-act/DataModified/expanded_database_ecommerce\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfbdae4c-4fd2-4bb3-b4d4-0c3173b6253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- parent_category: string (nullable = true)\n",
      " |-- subcategory: string (nullable = true)\n",
      " |-- price_bucket: string (nullable = true)\n",
      " |-- event_time_ts: timestamp (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.selectExpr(\n",
    "    'cast(event_time as timestamp) event_time',\n",
    "    'cast(event_type as string) event_type',\n",
    "    'cast(product_id as int) product_id',\n",
    "    'cast(category_id as long) category_id',\n",
    "    'cast(brand as string) brand',\n",
    "    'cast(price as float) price',\n",
    "    'cast(user_id as int) user_id',\n",
    "    'cast(user_session as string) user_session',\n",
    "    'cast(parent_category as string) parent_category',\n",
    "    'cast(subcategory as string) subcategory',\n",
    "    'cast(price_bucket as string) price_bucket',\n",
    "    'cast(event_time_ts as timestamp) event_time_ts',\n",
    "    'cast(day_of_week as int) day_of_week',\n",
    "    'cast(is_weekend as boolean) is_weekend',\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ba621-241d-4b5e-bc6d-0b53a4bf6b4d",
   "metadata": {},
   "source": [
    "### **3. Preparación de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d1566-8b98-406b-9376-2ae38a58cff3",
   "metadata": {},
   "source": [
    "En esta etapa, se deberán de aplicar estrategias de corrección sobre los datos que integran a la muestra M que se ha preparado en el paso previo, de tal forma que de deje un conjunto M listo para ser procesado por los algoritmos de aprendizaje a aplicar. Para ello se deben de considerar pasos como: corrección de registros / columnas con valores nulos, identificación de valores atípicos, transformación de los tipos de datos, etc. Con lo anterior, se tendrá una muestra M pre-procesada.\n",
    "\n",
    "Etapa donde se pre-procesa la muestra M, para corregir formatos e inconsistencias de cualquier índole que tenga la muestra original. Se deberá de documentar los pasos que se implementen para resolver esta etapa.\n",
    "\n",
    "Pre-procesa la muestra M corrigiendo todos los formatos e inconsistencias con documentación detallada y justificada de cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e14b3c1-0c5f-4a1d-9d64-46adcf484949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(event_time=datetime.datetime(2019, 11, 17, 0, 0), event_type='view', product_id=5300440, category_id=2053013563173241677, brand='vitek', price=17.760000228881836, user_id=513341639, user_session='d9544029-2739-4d16-9cac-79650460d9f0', parent_category='None', subcategory='None', price_bucket='low', event_time_ts=datetime.datetime(2019, 11, 17, 5, 35, 32), day_of_week=1, is_weekend=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9e6c-96ba-490c-a8ef-77e809599c32",
   "metadata": {},
   "source": [
    "### **4. Preparación del conjunto de entrenamiento y prueba**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86686301-b0b6-4a13-b383-7f6c06ffbc23",
   "metadata": {},
   "source": [
    "Para esta etapa, la muestra M será divida en un conjunto de entrenamiento y prueba. Para ello, deberás proponer una técnica de muestreo que te permita construir el conjunto de entrenamiento y prueba minimizando el riesgo de inyección de sesgos. Ten en cuenta que, para este punto, deberás de tener en claro el porcentaje de división a utilizar, el cual se deberá de justificar.\n",
    "\n",
    "Preparación del conjunto de entrenamiento y prueba: código en la cual se construye el conjunto de entrenamiento y prueba para la experimentación, debidamente documentado el código.\n",
    "\n",
    "Construye y documenta de manera excelente el conjunto de entrenamiento y prueba, con una explicación clara y justificada de cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b1b858-2c48-43b6-b9ad-d3e4303bd1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(event_time=datetime.datetime(2019, 11, 17, 0, 0), event_type='view', product_id=5300440, category_id=2053013563173241677, brand='vitek', price=17.760000228881836, user_id=513341639, user_session='d9544029-2739-4d16-9cac-79650460d9f0', parent_category='None', subcategory='None', price_bucket='low', event_time_ts=datetime.datetime(2019, 11, 17, 5, 35, 32), day_of_week=1, is_weekend=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75a495-49d0-4bfc-9913-d2bb9c56617c",
   "metadata": {},
   "source": [
    "### **5. Construcción de modelos de aprendizaje supervisado y no supervisado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5034b5-6dcb-4eda-8d72-7462b89e5a55",
   "metadata": {},
   "source": [
    "Para este punto realizarás dos experimentos separados, dónde se aplicará un algoritmo de aprendizaje supervisado y uno de aprendizaje no supervisado sobre la muestra M. Para el caso de aprendizaje supervisado, se deberá de identificar cuál es la variable objetivo (columna) de aprendizaje, mientras que, para el caso de aprendizaje no supervisado, se debe de seleccionar todas las columnas que se desean considerar como características bajo las cuales se realizará el proceso de agrupamiento. Usando las implementaciones correspondientes de PySpark, se deberá de ejecutar el aprendizaje correspondiente a partir de la invocación de las funciones respectivas. Para este ejercicio, se deberá seleccionar un criterio básico para medir la calidad del resultado obtenido, dependiendo de cada tipo de aprendizaje implementado. La elección quedará a juicio de cada estudiante.\n",
    "\n",
    "Construcción de modelos de aprendizaje supervisado y no supervisado: se recomienda dividir en dos subsecciones, una donde se muestre como se entrena un algoritmo de aprendizaje supervisado, y otra para el entrenamiento de un algoritmo no supervisado. Se debe de documentar el código implementado con una breve discusión de resultados.\n",
    "\n",
    "Entrena y documenta de manera excelente ambos tipos de algoritmos, con una discusión detallada y clara de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c16f0edb-0300-44f4-b227-f41dfa6221eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(event_time=datetime.datetime(2019, 11, 17, 0, 0), event_type='view', product_id=5300440, category_id=2053013563173241677, brand='vitek', price=17.760000228881836, user_id=513341639, user_session='d9544029-2739-4d16-9cac-79650460d9f0', parent_category='None', subcategory='None', price_bucket='low', event_time_ts=datetime.datetime(2019, 11, 17, 5, 35, 32), day_of_week=1, is_weekend=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
