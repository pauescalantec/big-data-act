{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1dc28d-4e92-4571-8622-d9bac54ca7e8",
   "metadata": {},
   "source": [
    "<img src=\"https://global.utsa.edu/tec-partnership/images/logos/logotipo-horizontal-azul-transparente.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4f6e1-f302-4b51-9c25-81b4c3c5408b",
   "metadata": {},
   "source": [
    "## **Actividad 3 | Aprendizaje supervisado y no supervisado**\n",
    "### **Análisis de grandes volúmenes de datos (Gpo 10)**\n",
    "#### Tecnológico de Monterrey\n",
    "---\n",
    "*   NOMBRE: Paulina Escalante Campbell\n",
    "*   MATRÍCULA: A01191962\n",
    "---\n",
    "### **Objetivo**\n",
    "\n",
    "Aplicar algoritmos de aprendizaje supervisado y no supervisado mediante PySpark para la resolución de problemas en análisis de datos, fomentando el desarrollo de habilidades prácticas en el manejo y procesamiento eficiente de grandes conjuntos de datos.\n",
    "\n",
    "### **1. Introducción**\n",
    "\n",
    "En el espacio de Big Data, existen tres principales algoritmos de aprendizaje, supervisado, no supervisado y por refuerzo. Estos algoritmos son aplicados a muestras de la población y poder predecir y analizar cantidades enormes de datos. Esta actividad se enfoca en los primeros dos ya que son utilizados generalmente en PySpark.\n",
    "\n",
    "El aprendizaje **supervisado** es un modelo de aprendizaje automático donde los algoritmos tienen una definición muy simple y estricta de los datos a analizar y los resultados son conocidos y categorizados. Un ejemplo muy básico es el análisis de rayos-x, la entrada es una imagen de rayos-x y la salida es la interpretación de si existe una enfermedad o no. Es un modelo predictor donde ya se conocen los resultados. Algunos algoritmos representativos de este enfoque incluyen:\n",
    "- Regresión logística\n",
    "- Árboles de decisión\n",
    "- Random Forest\n",
    "- Máquinas de soporte vectorial (SVM)\n",
    "- Redes neuronales artificiales\n",
    "  \n",
    "En PySpark, se pueden implementar varios de estos algoritmos a través del módulo `pyspark.ml.classification` y `pyspark.ml.regression`. Por ejemplo, PySpark ofrece soporte nativo para `LogisticRegression`, `DecisionTreeClassifier`, `RandomForestClassifier`, y `GBTClassifier`, entre otros.\n",
    "\n",
    "El aprendizaje **no supervisado** es un poco más complejo ya que se usan datos no estructurados y el resultado final no es conocido de antemano.  Se usa normalmente para identificar grupos de datos o categorías. Depende mucho de los datos y las relaciones escondidas que existen entre ellos. Un ejemplo es el uso de estos algoritmos en marketing en e-commerce. Empresas como Amazon y Walmart usan estos algoritmos para predecir lo que un cliente va a comprar a partir de un producto en su carrito, si alguien compra harina, el algoritmo podría predecir basado en la demográfica del cliente que el cliente quiere manzanas o en otros casos cebolla. Algunos de los algoritmos más representativos incluyen:\n",
    "- K-means\n",
    "- Clustering jerárquico\n",
    "- DBSCAN\n",
    "- Modelos de mezcla gaussiana\n",
    "- Análisis de componentes principales (PCA)\n",
    "\n",
    "En el ecosistema de PySpark, estos métodos están disponibles en el módulo `pyspark.ml.clustering` (por ejemplo, KMeans) y `pyspark.ml.feature` (para técnicas como PCA).\n",
    "\n",
    "#### **Tabla comparativa**\n",
    "\n",
    "| Característica                | Aprendizaje Supervisado                                 | Aprendizaje No Supervisado                                 |\n",
    "|----|----------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Definición**               | El modelo aprende a partir de datos etiquetados          | El modelo explora datos sin etiquetas para encontrar patrones |\n",
    "| **Objetivo principal**       | Predecir una etiqueta o valor conocido                   | Descubrir patrones, grupos o asociaciones         |\n",
    "| **Ejemplos de uso**          | Diagnóstico médico, detección de fraude | Segmentación de clientes, análisis de comportamiento |\n",
    "| **Algoritmos comunes**       | Regresión logística, árboles de decisión, random forest  | K-means, clustering jerárquico, PCA                         |\n",
    "| **PySpark**| `LogisticRegression`, `RandomForestClassifier`, etc. | `KMeans`, `PCA`                                 |\n",
    "| **Tipo de datos necesario**  | Datos con variables de entrada y una etiqueta de salida conocida | Solo variables de entrada, sin etiquetas                     |\n",
    "\n",
    "Por último, un ejemplo típico de aprendizaje no supervisado es la segmentación de clientes en marketing digital. A través del análisis de patrones de compra y navegación, es posible identificar diferentes grupos de clientes con comportamientos similares, lo que permite personalizar marketing y otras recomendaciones. Por otro lado, un ejemplo típico para aprendizajo supervisado sería un modelo que puede predecir si un correo es spam o no spam.\n",
    "\n",
    "#### **Supervisado vs. No Supervisado**\n",
    "![](https://lakshaysuri.wordpress.com/wp-content/uploads/2017/03/sup-vs-unsup.png?w=448)\n",
    "\n",
    "A continuación, usando el dataset de e-commerce usado en actividades previas podremos explorar lo que es aprendizajo supervisado y no supervisado y cómo prepar los datos para tener un modelo robusto y preciso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd792de0-251b-49ad-8da2-6e400013c66a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "815ab9f0-6b99-478d-8037-03433f3584f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, sum, when, split, col, lit, max, min, expr\n",
    "from pyspark.sql.functions import to_date, var_samp, variance, var_pop, month, to_timestamp, dayofweek\n",
    "from pyspark.sql.functions import hour, month\n",
    "from pyspark.sql.types import NumericType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, round, concat_ws, desc, when, concat\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a54271-e1dc-4cc9-8c31-814bc73a46ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **2. Selección de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42191c-b31e-4dd4-ad7a-20070b5c2026",
   "metadata": {},
   "source": [
    "Para esta actividad, se propone recolectar una muestra de dimensión contenida (para evitar que los tiempos de procesamiento sean altos) a partir de la base de datos que estás trabajando en tu proyecto. Para ello y tomando como base la actividad previa en la cual has implementado códigos que permiten obtener particiones de la base de datos global D que cumplen con los criterios de las variables de caracterización identificadas, se propone que recuperes un número limitado de instancias de cada partición (aplicando la técnica de muestreo que propusiste en el Módulo 3, Proyecto Base de datos de Big Data, paso 4), lo que te permitirá construir una muestra M a partir de la unión de las instancias que se recuperan de este proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e9097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/25 20:29:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/25 20:29:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Maestria_evidencia1\") \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"16g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Mejores tablas\n",
    "#spark, comentando el comando del environment para reducir el ruido del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbbad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 20:29:40 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 3:=================================================>    (134 + 12) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de registros: 109950731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "####\n",
    "#### La base de datos ha sido limpiada y modificada a este punto\n",
    "#### \n",
    "####\n",
    "file_path = \"/Users/pauescalante/Documents/Maestria/Trimestre 7/BigData/big-data-act/DataModified/expanded_database_ecommerce\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Mostrar cuantos registros se tienen inicialmete para en el futuro reducir a una dimensión contenida\n",
    "initial_total_count = df.count()\n",
    "print(f\"Número total de registros: {initial_total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbdae4c-4fd2-4bb3-b4d4-0c3173b6253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: date (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_time_ts: timestamp (nullable = true)\n",
      " |-- parent_category: string (nullable = true)\n",
      " |-- subcategory: string (nullable = true)\n",
      " |-- price_bucket: string (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imprimiendo el esquema del Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef24d0a1-0b83-44af-96f3-034dd4aca558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------\n",
      " event_time      | 2019-11-17                           \n",
      " event_type      | view                                 \n",
      " product_id      | 5300440                              \n",
      " category_id     | 2053013563173241677                  \n",
      " brand           | vitek                                \n",
      " price           | 17.76                                \n",
      " user_id         | 513341639                            \n",
      " user_session    | d9544029-2739-4d16-9cac-79650460d9f0 \n",
      " event_time_ts   | 2019-11-17 05:35:32                  \n",
      " parent_category | None                                 \n",
      " subcategory     | None                                 \n",
      " price_bucket    | low                                  \n",
      " day_of_week     | 1                                    \n",
      " is_weekend      | true                                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el primer registros de ejemplo para visualizar las columnas\n",
    "df.show(n=1,truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ba621-241d-4b5e-bc6d-0b53a4bf6b4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **3. Preparación de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d1566-8b98-406b-9376-2ae38a58cff3",
   "metadata": {},
   "source": [
    "En esta etapa, se deberán de aplicar estrategias de corrección sobre los datos que integran a la muestra M que se ha preparado en el paso previo, de tal forma que de deje un conjunto M listo para ser procesado por los algoritmos de aprendizaje a aplicar. Para ello se deben de considerar pasos como: corrección de registros / columnas con valores nulos, identificación de valores atípicos, transformación de los tipos de datos, etc. Con lo anterior, se tendrá una muestra M pre-procesada.\n",
    "\n",
    "Etapa donde se pre-procesa la muestra M, para corregir formatos e inconsistencias de cualquier índole que tenga la muestra original. Se deberá de documentar los pasos que se implementen para resolver esta etapa.\n",
    "\n",
    "Pre-procesa la muestra M corrigiendo todos los formatos e inconsistencias con documentación detallada y justificada de cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a122615-1979-474d-a1d0-396fac970bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Análisis de valores nulos ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'event_time' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'event_type' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'product_id' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'category_id' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'brand' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'price' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'user_id' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'user_session' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'event_time_ts' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'parent_category' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'subcategory' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'price_bucket' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'day_of_week' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:=====================================================>(144 + 2) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna 'is_weekend' no tiene valores null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# En este punto en actividades previas se limpiaron un poco los datos, para validar que no hay nulls\n",
    "print(\"\\n--- Análisis de valores nulos ---\")\n",
    "for column in df.columns:\n",
    "    count = df.filter(col(column).isNull()).count()\n",
    "    if count > 0:\n",
    "        print(f\"Columna '{column}' tiene {count} valores null\")\n",
    "    else:\n",
    "        print(f\"Columna '{column}' no tiene valores null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97d02a7-a607-4109-808a-0a34f9b24ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Análisis de valores atípicos (precio) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:=====================================================>(144 + 2) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            price|\n",
      "+-------+-----------------+\n",
      "|  count|        109950731|\n",
      "|   mean|291.6348043534796|\n",
      "| stddev|356.6799794159954|\n",
      "|    min|              0.0|\n",
      "|    max|          2574.07|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Análisis de valores atípicos en precio, para justificar usar price_bucket en vez de precio\n",
    "print(\"\\n--- Análisis de valores atípicos (precio) ---\")\n",
    "p_stats = df.select(\"price\").describe()\n",
    "p_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "133a2bca-3e64-48b2-a042-38bca58e4254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:=================================================>   (136 + 10) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Análisis de outliers en precio: ---\n",
      "Rango normal: $-336.79 - $734.97\n",
      "Outliers: 11,488,995 (10.45%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Ya que tenemos una varianza alta en precios, podemos observar los cuartiles para entender un poco mas los outliers del precio\n",
    "# Calcular outliers usando IQR\n",
    "quantiles = df.select(\"price\").approxQuantile(\"price\", [0.25, 0.75], 0.05)\n",
    "Q1, Q3 = quantiles[0], quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_count = df.filter((col(\"price\") < lower_bound) | (col(\"price\") > upper_bound)).count()\n",
    "outlier_pct = (outliers_count / df.count()) * 100\n",
    "\n",
    "# Esta alta dispersión justifica técnicamente el uso de price_bucket en lugar del precio absoluto,\n",
    "# ya que las categorías Low/Medium/High capturan mejor los segmentos de mercado naturales y mejorarán\n",
    "# el rendimiento de los modelos de machine learning\n",
    "print(\"\\n--- Análisis de outliers en precio: ---\")\n",
    "print(f\"Rango normal: ${lower_bound:.2f} - ${upper_bound:.2f}\")\n",
    "print(f\"Outliers: {outliers_count:,} ({outlier_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b46054b-9164-4e74-a769-e738ea4e0533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Distribución completa price_bucket: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=====================================================>(144 + 2) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+\n",
      "|price_bucket|   count|percentage|\n",
      "+------------+--------+----------+\n",
      "|        high|32721216|     29.76|\n",
      "|         low|37649697|     34.24|\n",
      "|      medium|39579818|      36.0|\n",
      "+------------+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Distribución de price_bucket en porcentajes antes del sampling para verificar la muestra\n",
    "price_bucket_distribution = (\n",
    "    df.groupBy(\"price_bucket\")\n",
    "    .count()\n",
    "    .withColumn(\"total\", lit(initial_total_count))\n",
    "    .withColumn(\"percentage\", round((col(\"count\") / col(\"total\")) * 100, 2))\n",
    "    .select(\"price_bucket\", \"count\", \"percentage\")\n",
    "    .orderBy(\"price_bucket\")\n",
    ")\n",
    "\n",
    "print(\"\\n--- Distribución completa price_bucket: ---\")\n",
    "price_bucket_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc20bb7d-5671-46ca-bbf8-81672ec41bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Distribución completa event_type: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:=====================================================>(144 + 2) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|event_type|    count|percentage|\n",
      "+----------+---------+----------+\n",
      "|      cart|  3955434|       3.6|\n",
      "|  purchase|  1659788|      1.51|\n",
      "|      view|104335509|     94.89|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Distribución de event_type en porcentajes antes del sampling\n",
    "event_type_distribution = (\n",
    "    df.groupBy(\"event_type\")\n",
    "    .count()\n",
    "    .withColumn(\"total\", lit(initial_total_count))\n",
    "    .withColumn(\"percentage\", round((col(\"count\") / col(\"total\")) * 100, 2))\n",
    "    .select(\"event_type\", \"count\", \"percentage\")\n",
    "    .orderBy(\"event_type\")\n",
    ")\n",
    "\n",
    "# La distribución de estos valores también es esparada ya que no muchos clientes compran pero muchos visitan la página web\n",
    "print(\"\\n--- Distribución completa event_type: ---\")\n",
    "event_type_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48eb2e2-8bda-4d9e-bcd6-1c5fc4c6aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SRS sampling: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:================================================>    (133 + 12) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sample size: 10861)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Calcular muestras M pre-procesada, en la actividad anterior definimos que SRS (Simple Random Sampling)\n",
    "# Ya que tenemos una poblacion muy grande (approx. 109950731, podemos usar un 0.01% de muestra y tener un número significativo de datos)\n",
    "print(\"\\n--- SRS sampling: ---\")\n",
    "sample_df = df.sample(fraction=0.0001)\n",
    "total_count_sample = sample_df.count()\n",
    "print(f\"New sample size: {total_count_sample})\")\n",
    "\n",
    "#Variables de Caracterización Seleccionadas\n",
    "#event_type: Representa el funnel de conversión (view → cart → purchase)\n",
    "#price_bucket: Segmenta productos por rango de precio (low/medium/high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef844a1d-751f-44ee-b380-8d628bb32c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Distribución del sample price_bucket: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+\n",
      "|price_bucket|count|percentage|\n",
      "+------------+-----+----------+\n",
      "|        high| 3225|     29.69|\n",
      "|         low| 3721|     34.26|\n",
      "|      medium| 3915|     36.05|\n",
      "+------------+-----+----------+\n",
      "\n",
      "\n",
      "--- Distribución del sample event_type: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:=====================================================>(144 + 2) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n",
      "|event_type|count|percentage|\n",
      "+----------+-----+----------+\n",
      "|      cart|  398|      3.66|\n",
      "|  purchase|  154|      1.42|\n",
      "|      view|10309|     94.92|\n",
      "+----------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Calcular distribuciónes nuevamente del sample\n",
    "sample_price_bucket_distribution = (\n",
    "    sample_df.groupBy(\"price_bucket\")\n",
    "    .count()\n",
    "    .withColumn(\"total\", lit(total_count_sample))\n",
    "    .withColumn(\"percentage\", round((col(\"count\") / col(\"total\")) * 100, 2))\n",
    "    .select(\"price_bucket\", \"count\", \"percentage\")\n",
    "    .orderBy(\"price_bucket\")\n",
    ")\n",
    "\n",
    "sample_event_type_distribution = (\n",
    "    sample_df.groupBy(\"event_type\")\n",
    "    .count()\n",
    "    .withColumn(\"total\", lit(total_count_sample))\n",
    "    .withColumn(\"percentage\", round((col(\"count\") / col(\"total\")) * 100, 2))\n",
    "    .select(\"event_type\", \"count\", \"percentage\")\n",
    "    .orderBy(\"event_type\")\n",
    ")\n",
    "\n",
    "# Se puede observar que el sample tiene distribuciónes similares a la poblacion\n",
    "# Los nuevos valores son sample_df y total_count_sample de ahora en adelante\n",
    "print(\"\\n--- Distribución del sample price_bucket: ---\")\n",
    "sample_price_bucket_distribution.show()\n",
    "\n",
    "print(\"\\n--- Distribución del sample event_type: ---\")\n",
    "sample_event_type_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b1b858-2c48-43b6-b9ad-d3e4303bd1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:=====================================================>(144 + 2) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columna: 'subcategory' — 114 valores distintos\n",
      "--------------------------------------------------\n",
      "\n",
      "Columna: 'parent_category' — 14 valores distintos\n",
      "--------------------------------------------------\n",
      "\n",
      "Columna: 'brand' — 960 valores distintos\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Valores distintos de columnas categorícas para ver si se puede normalizar o agrupar mas información \n",
    "categoricas_columnas= ['subcategory', 'parent_category', 'brand']\n",
    "output = \"\"\n",
    "for column in categoricas_columnas:\n",
    "    # Cuantos valores existen\n",
    "    distinct_count = sample_df.select(column).distinct().count()\n",
    "    \n",
    "    output += f\"\\nColumna: '{column}' — {distinct_count} valores distintos\\n\"\n",
    "    output += \"-\" * 50 + \"\\n\"\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa7bf5c1-8eff-43eb-83a5-d7b17cf7d4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Reducir variabilidad de brand del sample: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:=====================================================>(144 + 2) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columna brand: — 32 valores distintos\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Para brands, si se tienen menos de 50 registros, usamos \"Others\" asi reducimos variabilidad de brands\n",
    "min_count = 50\n",
    "brands_over_min_count = (\n",
    "    sample_df.groupBy(\"brand\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") >= min_count)\n",
    "    .select(\"brand\")\n",
    ")\n",
    "\n",
    "# Usando esta lista hacemos un filtering\n",
    "brands_list = [row[\"brand\"] for row in brands_over_min_count.collect()]\n",
    "\n",
    "sample_df = sample_df.withColumn(\n",
    "    \"brand\",\n",
    "    when(col(\"brand\").isin(brands_list), col(\"brand\")).otherwise(\"others\")\n",
    ")\n",
    "\n",
    "print(\"\\n--- Reducir variabilidad de brand del sample: ---\")\n",
    "# Cuantos valores existen en el nuevo df, reducimos de 2194 a 294\n",
    "# Es una manera de normalizar el valor cetagórico de brand\n",
    "distinct_count_new = sample_df.select(\"brand\").distinct().count()\n",
    "print(f\"\\nColumna brand: — {distinct_count_new} valores distintos\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9e6c-96ba-490c-a8ef-77e809599c32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4. Preparación del conjunto de entrenamiento y prueba**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86686301-b0b6-4a13-b383-7f6c06ffbc23",
   "metadata": {},
   "source": [
    "Para esta etapa, la muestra M será divida en un conjunto de entrenamiento y prueba. Para ello, deberás proponer una técnica de muestreo que te permita construir el conjunto de entrenamiento y prueba minimizando el riesgo de inyección de sesgos. Ten en cuenta que, para este punto, deberás de tener en claro el porcentaje de división a utilizar, el cual se deberá de justificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46265fd1-050d-4408-9622-a4156362ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Entrenamiento/Prueba ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:====================================================>(144 + 2) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen 8667 instancias en el conjunto train, y 2194 en el conjunto test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# sample_df tiene la muestra de la población del data set original\n",
    "# Para separar la muestra entre sets de entrenamiento y prueba se usa un 80:20\n",
    "# Es el estándar en ML y un buen balance\n",
    "print(\"\\n-- Entrenamiento/Prueba ---\")\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "random_seed = 42\n",
    "\n",
    "# Establece el número de particiones que se usarán al hacer operaciones como shuffle (por ejemplo, en joins, agregaciones o splits).\n",
    "# Un número mayor puede mejorar la distribución de los datos en clústeres grandes, pero también aumentar el uso de recursos.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\") # puede alterar los resultados, empezamos con un número mayor\n",
    "\n",
    "# Divide aleatoriamente el DataFrame `sample_df` en dos subconjuntos: uno para entrenamiento y otro para prueba.\n",
    "train_data,test_data = sample_df.randomSplit([train_ratio,test_ratio], seed = random_seed)\n",
    "\n",
    "# Imprime cuántas instancias hay en el conjunto de entrenamiento y cuántas en el conjunto de prueba.\n",
    "print(f\"\"\"Existen {train_data.count()} instancias en el conjunto train, y {test_data.count()} en el conjunto test\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5884b-29c0-45bb-a3ba-1c77e9b05bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verificación de proporciones event_type: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:===================================================> (143 + 3) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "|event_type|train_count|test_count|\n",
      "+----------+-----------+----------+\n",
      "|  purchase|        119|        35|\n",
      "|      view|       8228|      2081|\n",
      "|      cart|        320|        78|\n",
      "+----------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Verificar proporciones de los sets de training y de test con event_type, ya que esta columna tiene una distribución esperada\n",
    "train_dist = train_data.groupBy(\"event_type\").count().withColumnRenamed(\"count\", \"train_count\")\n",
    "test_dist = test_data.groupBy(\"event_type\").count().withColumnRenamed(\"count\", \"test_count\")\n",
    "verification = train_dist.join(test_dist, \"event_type\")\n",
    "\n",
    "# Las distribuciones son esperadas, con view > cart > purchase\n",
    "print(\"\\n--- Verificación de proporciones event_type: ---\")\n",
    "verification.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0746999-64fd-43fd-abe7-80c8850e962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verificación de proporciones price_bucket: ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:===================================================> (142 + 4) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------+\n",
      "|price_bucket|train_count|test_count|\n",
      "+------------+-----------+----------+\n",
      "|         low|       2996|       725|\n",
      "|        high|       2566|       659|\n",
      "|      medium|       3105|       810|\n",
      "+------------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Verificar proporciones de los sets de training y de test con event_type, ya que esta columna tiene una distribución normal\n",
    "train_dist_2 = train_data.groupBy(\"price_bucket\").count().withColumnRenamed(\"count\", \"train_count\")\n",
    "test_dist_2 = test_data.groupBy(\"price_bucket\").count().withColumnRenamed(\"count\", \"test_count\")\n",
    "verification_2 = train_dist_2.join(test_dist_2, \"price_bucket\")\n",
    "\n",
    "# Las distribuciones son esperadas, con un 33% aproximado en cada categoría \n",
    "print(\"\\n--- Verificación de proporciones price_bucket: ---\")\n",
    "verification_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75a495-49d0-4bfc-9913-d2bb9c56617c",
   "metadata": {},
   "source": [
    "### **5. Construcción de modelos de aprendizaje supervisado y no supervisado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5034b5-6dcb-4eda-8d72-7462b89e5a55",
   "metadata": {},
   "source": [
    "Para este punto realizarás dos experimentos separados, dónde se aplicará un algoritmo de aprendizaje supervisado y uno de aprendizaje no supervisado sobre la muestra M. Para el caso de aprendizaje supervisado, se deberá de identificar cuál es la variable objetivo (columna) de aprendizaje, mientras que, para el caso de aprendizaje no supervisado, se debe de seleccionar todas las columnas que se desean considerar como características bajo las cuales se realizará el proceso de agrupamiento. Usando las implementaciones correspondientes de PySpark, se deberá de ejecutar el aprendizaje correspondiente a partir de la invocación de las funciones respectivas. Para este ejercicio, se deberá seleccionar un criterio básico para medir la calidad del resultado obtenido, dependiendo de cada tipo de aprendizaje implementado. La elección quedará a juicio de cada estudiante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df58a9-bd1e-4b3b-b7d4-8d8a33c56466",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.1 Aprendizaje Supervisado\n",
    "\n",
    "Para el problema de aprendizaje supervisado haremos un análisis binario de predicción: compra o no compra\n",
    "\n",
    "- Variable objetivo: label (1 = purchase, 0 = no purchase)\n",
    "- Desafío: Dataset altamente desbalanceado (1.5% positivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c768e8e-1f20-4dca-89ef-23076ce82426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable objetivo: label (1=purchase, 0=no purchase)\n",
      "Características numéricas: ['price', 'day_of_week']\n",
      "Características categóricas: ['brand', 'parent_category', 'price_bucket']\n",
      "+----------+-----------+\n",
      "|event_time|day_of_week|\n",
      "+----------+-----------+\n",
      "|2019-11-17|          1|\n",
      "|2019-11-17|          1|\n",
      "|2019-11-17|          1|\n",
      "|2019-11-17|          1|\n",
      "|2019-11-17|          1|\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparar la variable objetivo \"label\" que identifica si es compra o no\n",
    "supervised_df = train_data.withColumn(\"label\", \n",
    "                                   when(col(\"event_type\") == \"purchase\", 1.0)\n",
    "                                   .otherwise(0.0))\n",
    "\n",
    "print(\"Variable objetivo: label (1=purchase, 0=no purchase)\")\n",
    "\n",
    "# Seleccionar características, en este caso tenemos variables numéricas y categóricas\n",
    "feature_cols = [\"price\", \"day_of_week\"]\n",
    "categorical_cols = [\"brand\", \"parent_category\", \"price_bucket\"]\n",
    "\n",
    "print(f\"Características numéricas: {feature_cols}\")\n",
    "print(f\"Características categóricas: {categorical_cols}\")\n",
    "\n",
    "# Revisar resultado\n",
    "sample_df.select(\"event_time\", \"day_of_week\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "306c8e06-0fe8-4d57-bfd4-5443cf317181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación del dataframe para ser procesado con algoritmos de ML en PySpark\n",
    "# Se usa VectorAssembler para generar una o más columnas en la cual, se \"encapsulan\" en un vector único\n",
    "# los valores de los descriptores a usar en el proceso de aprendizaje.\n",
    "\n",
    "# Indexar las variables categóricas (String → Numérico).\n",
    "# Se crea una lista de transformadores StringIndexer, uno por cada columna categórica.\n",
    "# `handleInvalid=\"skip\"` evita errores si hay valores nulos o inesperados.\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"skip\") \n",
    "            for col in categorical_cols]\n",
    "\n",
    "# Ensamblar todas las variables numéricas y categóricas indexadas en una sola columna de características.\n",
    "# Esto es necesario porque PySpark ML requiere una sola columna de entrada (`features`) de tipo vector.\n",
    "feature_cols_final = feature_cols + [f\"{col}_indexed\" for col in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols_final, outputCol=\"features\")\n",
    "\n",
    "# Escalar las características para normalizar los valores.\n",
    "# Esto mejora el rendimiento de muchos algoritmos de ML\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70542e10-31ae-4956-9a42-06a3a3de5eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Modelo 1: Regresión Logística ---\n",
      "Entrenando Regresión Logística...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 21:59:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/25 21:59:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado...\n"
     ]
    }
   ],
   "source": [
    "# MODELO 1: Regresión Logística\n",
    "# Baseline estándar para clasificación binaria\n",
    "print(\"\\n--- Modelo 1: Regresión Logística ---\")\n",
    "\n",
    "# Se especifican las columnas de entrada (`scaledFeatures`) y etiqueta (`label`), y el número máximo de iteraciones.\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Se construye un pipeline que incluye:\n",
    "# 1. Indexado de columnas categóricas\n",
    "# 2. Ensamblado de características\n",
    "# 3. Escalado de variables\n",
    "# 4. El modelo de regresión logística\n",
    "lr_pipeline = Pipeline(stages=indexers + [assembler, scaler, lr])\n",
    "\n",
    "print(\"Entrenando Regresión Logística...\")\n",
    "\n",
    "# Se entrena el pipeline completo con el DataFrame `supervised_df`.\n",
    "lr_model = lr_pipeline.fit(supervised_df)\n",
    "print(\"Modelo entrenado...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa436620-d081-465f-8329-f49b1fb881b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficient of the model is : [-0.08414246503215402,-0.10305213628995627,-0.011979846792161571,-0.45523631995847075,-0.058556795214095445]\n",
      "The intercept of the model is : -3.657629793952262\n"
     ]
    }
   ],
   "source": [
    "# Se Imprimen los valores de los coeficientes\n",
    "lr_stage = lr_model.stages[-1]  # Última etapa es LogisticRegression\n",
    "print(\"The coefficient of the model is :\", lr_stage.coefficients)\n",
    "print(\"The intercept of the model is :\", lr_stage.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "193065bb-bcf2-4663-b8b9-67fa860aba7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluación de modelos ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 195:====================================================>(144 + 2) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados:\n",
      "Regresión Logística - AUC: 0.5571, Accuracy: 0.9840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluación de modelos ---\")\n",
    "# Preparar datos de prueba con la variable label\n",
    "test_supervised = test_data.withColumn(\"label\", \n",
    "                                    when(col(\"event_type\") == \"purchase\", 1.0)\n",
    "                                    .otherwise(0.0))\n",
    "\n",
    "# Generar predicciones con el modelo de regresión logística previamente entrenado.\n",
    "lr_predictions = lr_model.transform(test_supervised)\n",
    "\n",
    "# Crear un evaluador para clasificación binaria.\n",
    "# Se usará el área bajo la curva ROC (AUC) como métrica principal para evaluar qué tan bien distingue entre 0 y 1.\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Crear un evaluador adicional para calcular la precisión general (accuracy).\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                                      predictionCol=\"prediction\", \n",
    "                                                      metricName=\"accuracy\")\n",
    "\n",
    "# Métricas para evaluar\n",
    "lr_auc = binary_evaluator.evaluate(lr_predictions)\n",
    "lr_accuracy = accuracy_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"\\nResultados:\")\n",
    "print(f\"Regresión Logística - AUC: {lr_auc:.4f}, Accuracy: {lr_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca705c2b-b934-4b5f-9d58-3f9c5f867706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusión:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:====================================================>(145 + 1) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       0.0|   35|\n",
      "|  0.0|       0.0| 2159|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Matriz de confusión\n",
    "print(\"\\nMatriz de confusión:\")\n",
    "confusion_matrix = lr_predictions.groupBy(\"label\", \"prediction\").count()\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c22f9-6b87-465e-82e0-4123c91cf374",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.2 Aprendizaje No Supervisado\n",
    "\n",
    "Para el problema de aprendizaje no supervisado haremos un análisis de clustering: agrupando productos por precio y popularidad\n",
    "\n",
    "- Segmentación de prodcutos: Identificar productos populares y sus precios\n",
    "- Objetivo: Marketing personalizado y estrategias diferenciadas\n",
    "- Enfoque: Clustering basado en patrones de comportamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c16f0edb-0300-44f4-b227-f41dfa6221eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Productos para clustering: 68\n",
      "Muestra de perfiles de productos:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 206:====================================================>(145 + 1) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "|product_id|         avg_price|total_views|cart_count|purchase_count|popularity_score|     conversion_rate|\n",
      "+----------+------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "|  22700129| 68.10166666666666|         12|         0|             0|              12|                 0.0|\n",
      "|   1004659| 727.0933333333334|         21|         0|             1|              26|0.047619047619047616|\n",
      "|   3700926| 67.98583333333333|         12|         2|             1|              21| 0.08333333333333333|\n",
      "|   1005098|         139.95875|         16|         2|             0|              20|                 0.0|\n",
      "|   1004781|263.97714285714284|         14|         1|             0|              16|                 0.0|\n",
      "|   5100816|29.675238095238093|         21|         1|             2|              33| 0.09523809523809523|\n",
      "|   1005105| 1366.540930232558|         43|         2|             1|              52|0.023255813953488372|\n",
      "|   1004840| 948.5140000000001|         10|         0|             1|              15|                 0.1|\n",
      "|   1004249| 752.7459999999999|         40|         2|             1|              49|               0.025|\n",
      "|   4804056|161.30774193548388|         62|         8|             2|              88| 0.03225806451612903|\n",
      "+----------+------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Características seleccionadas: ['avg_price', 'total_views', 'popularity_score']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Agrupa por `product_id` y calcula métricas agregadas:\n",
    "# - Precio promedio del producto\n",
    "# - Total de interacciones (vistas, carritos, compras)\n",
    "# - Número de veces que el producto fue añadido al carrito\n",
    "# - Número de veces que fue comprado\n",
    "product_profiles = train_data.groupBy(\"product_id\").agg(\n",
    "    F.avg(\"price\").alias(\"avg_price\"),\n",
    "    F.count(\"*\").alias(\"total_views\"),\n",
    "    F.sum(when(col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"cart_count\"),\n",
    "    F.sum(when(col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchase_count\")\n",
    ").filter(col(\"total_views\") >= 10)  # Filtra productos con al menos 10 interacciones para reducir el ruido de clientes no frecuentes\n",
    "\n",
    "# Agrega métricas derivadas de popularidad:\n",
    "# - popularity_score: puntuación basada en interacción (views + 2*carritos + 5*compras)\n",
    "# - conversion_rate: tasa de conversión (compras / vistas totales)\n",
    "product_profiles = product_profiles.withColumn(\n",
    "    \"popularity_score\", col(\"total_views\") + col(\"cart_count\") * 2 + col(\"purchase_count\") * 5\n",
    ").withColumn(\n",
    "    \"conversion_rate\", col(\"purchase_count\") / col(\"total_views\")\n",
    ")\n",
    "\n",
    "# Cuántos productos se usarán para el clustering y una muestra de ellos\n",
    "print(f\"Productos para clustering: {product_profiles.count():,}\")\n",
    "print(\"Muestra de perfiles de productos:\")\n",
    "product_profiles.show(10)\n",
    "\n",
    "# Selecciona las columnas que se usarán como características numéricas para el clustering\n",
    "clustering_features = [\"avg_price\", \"total_views\", \"popularity_score\"]\n",
    "print(f\"Características seleccionadas: {clustering_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41289ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo K-Means...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 250:====================================================>(145 + 1) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo completado...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Construye un pipeline de clustering en tres etapas:\n",
    "# 1. VectorAssembler: combina las columnas numéricas en una sola columna de vectores\n",
    "# 2. StandardScaler: normaliza los valores numéricos\n",
    "# 3.KMeans: aplica el algoritmo de agrupamiento con 4 clusters\n",
    "assembler = VectorAssembler(inputCols=clustering_features, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\", k=4, seed=42)\n",
    "\n",
    "# Construye el pipeline y lo ajusta (fit) al DataFrame `product_profiles`\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "print(\"Entrenando modelo K-Means...\")\n",
    "model = pipeline.fit(product_profiles)\n",
    "\n",
    "# Aplica el modelo de clustering a los perfiles de productos para asignar un clúster a cada producto\n",
    "clustered_products = model.transform(product_profiles)\n",
    "print(\"Modelo completado...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7519158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Análisis de Clusters ---\n",
      "Resumen por cluster:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 256:====================================================>(145 + 1) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+------------------+--------------------+\n",
      "|prediction|num_products| avg_price_cluster|    avg_popularity|      avg_conversion|\n",
      "+----------+------------+------------------+------------------+--------------------+\n",
      "|         0|          16| 864.8541590319077|           17.0625| 0.01933768750629216|\n",
      "|         1|           4|370.78704276139734|             73.25| 0.03753284192088746|\n",
      "|         2|          41|218.95489361534854|14.292682926829269| 0.02686344143661217|\n",
      "|         3|           7| 354.5078094808635|              38.0|0.022876542737169915|\n",
      "+----------+------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Análisis de Clusters ---\")\n",
    "\n",
    "# Estadísticas por cluster\n",
    "cluster_summary = clustered_products.groupBy(\"prediction\").agg(\n",
    "    F.count(\"*\").alias(\"num_products\"),\n",
    "    F.avg(\"avg_price\").alias(\"avg_price_cluster\"),\n",
    "    F.avg(\"total_views\").alias(\"avg_popularity\"),\n",
    "    F.avg(\"conversion_rate\").alias(\"avg_conversion\")\n",
    ").orderBy(\"prediction\")\n",
    "\n",
    "print(\"Resumen por cluster:\")\n",
    "cluster_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39dcca8f-7559-4501-917d-65097d82ad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 262:====================================================>(145 + 1) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0: 🏆 Premium\n",
      "  • Productos: 16\n",
      "  • Precio promedio: $864.85\n",
      "  • Popularidad: 17.1 views\n",
      "  • Conversión: 0.019 (1.9%)\n",
      "  • Productos caros, baja popularidad, alta conversión\n",
      "\n",
      "Cluster 1: 📦 Basics\n",
      "  • Productos: 4\n",
      "  • Precio promedio: $370.79\n",
      "  • Popularidad: 73.2 views\n",
      "  • Conversión: 0.038 (3.8%)\n",
      "  • Actividad normal, precios accesibles\n",
      "\n",
      "Cluster 2: 📦 Basics\n",
      "  • Productos: 41\n",
      "  • Precio promedio: $218.95\n",
      "  • Popularidad: 14.3 views\n",
      "  • Conversión: 0.027 (2.7%)\n",
      "  • Actividad normal, precios accesibles\n",
      "\n",
      "Cluster 3: 📦 Basics\n",
      "  • Productos: 7\n",
      "  • Precio promedio: $354.51\n",
      "  • Popularidad: 38.0 views\n",
      "  • Conversión: 0.023 (2.3%)\n",
      "  • Actividad normal, precios accesibles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Se recopilan los resultados agregados de los clústeres desde un DataFrame Spark a una lista local de Python.\n",
    "cluster_data = cluster_summary.collect()\n",
    "\n",
    "# Se itera sobre cada fila del resumen de clústeres para analizar y clasificar los grupos\n",
    "for row in cluster_data:\n",
    "    cluster_id = int(row['prediction'])\n",
    "    count = int(row['num_products'])\n",
    "    price = float(row['avg_price_cluster'])\n",
    "    popularity = float(row['avg_popularity'])\n",
    "    conversion = float(row['avg_conversion'])\n",
    "    \n",
    "    # Clasificación simple\n",
    "    if price > 500:\n",
    "        cluster_type = \"🏆 Premium\"\n",
    "        description = \"Productos caros, baja popularidad, alta conversión\"\n",
    "    elif popularity > 100:\n",
    "        cluster_type = \"🔥 Populares\"\n",
    "        description = \"Mucha actividad, precios medios\"\n",
    "    elif conversion > 0.05:\n",
    "        cluster_type = \"💰 Best Seller\"\n",
    "        description = \"Buena conversión, productos exitosos\"\n",
    "    else:\n",
    "        cluster_type = \"📦 Basics\"\n",
    "        description = \"Actividad normal, precios accesibles\"\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}: {cluster_type}\")\n",
    "    print(f\"  • Productos: {count:,}\")\n",
    "    print(f\"  • Precio promedio: ${price:.2f}\")\n",
    "    print(f\"  • Popularidad: {popularity:.1f} views\")\n",
    "    print(f\"  • Conversión: {conversion:.3f} ({conversion*100:.1f}%)\")\n",
    "    print(f\"  • {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0723dc80-0f2b-4ece-b6b1-c05a9471c52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LR guardado en: /Users/pauescalante/Documents/Maestria/Trimestre 7/BigData/big-data-act/Models/logistic_regression_model_20250525_223454\n",
      "Modelo K-Means guardado en: /Users/pauescalante/Documents/Maestria/Trimestre 7/BigData/big-data-act/Models/kmeans_model_20250525_223454\n"
     ]
    }
   ],
   "source": [
    "# Guardar los modelos\n",
    "base_path = \"/Users/pauescalante/Documents/Maestria/Trimestre 7/BigData/big-data-act/Models\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "lr_model_path = f\"{base_path}/logistic_regression_model_{timestamp}\"\n",
    "try:\n",
    "    lr_model.write().overwrite().save(lr_model_path)\n",
    "    print(f\"Modelo LR guardado en: {lr_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error guardando LR: {e}\")\n",
    "\n",
    "# Guardar modelo K-Means completo\n",
    "kmeans_model_path = f\"{base_path}/kmeans_model_{timestamp}\"\n",
    "try:\n",
    "    # Asumiendo que 'model' es tu pipeline de clustering\n",
    "    model.write().overwrite().save(kmeans_model_path)\n",
    "    print(f\"Modelo K-Means guardado en: {kmeans_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error guardando K-Means: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dcdec2-402d-449e-86fd-a43ccf60d9d1",
   "metadata": {},
   "source": [
    "### **6. Conclusión**\n",
    "\n",
    "En esta actividad se exploraron técnicas de aprendizaje supervisado y no supervisado utilizando PySpark sobre un conjunto de datos reales de comportamiento en e-commerce. En el caso supervisado, se entrenó un modelo de regresión logística para predecir la probabilidad de compra, enfrentando el reto de una distribución altamente desbalanceada. Para el aprendizaje no supervisado, se aplicó K-Means para segmentar productos en clústeres basados en métricas de precio, popularidad y conversión. Esta segmentación permite identificar oportunidades estratégicas para acciones de marketing y posicionamiento de productos. El uso de pipelines y transformaciones escalables con PySpark permitió construir un flujo de trabajo eficiente y reproducible para el análisis de grandes volúmenes de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
