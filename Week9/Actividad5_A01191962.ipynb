{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1dc28d-4e92-4571-8622-d9bac54ca7e8",
   "metadata": {},
   "source": [
    "<img src=\"https://global.utsa.edu/tec-partnership/images/logos/logotipo-horizontal-azul-transparente.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4f6e1-f302-4b51-9c25-81b4c3c5408b",
   "metadata": {},
   "source": [
    "## **Actividad 5 | Actividad de visualización de resultados**\n",
    "### **Análisis de grandes volúmenes de datos (Gpo 10)**\n",
    "#### Tecnológico de Monterrey\n",
    "---\n",
    "*   NOMBRE: Paulina Escalante Campbell\n",
    "*   MATRÍCULA: A01191962\n",
    "---\n",
    "### **Objetivo**\n",
    "Mostrar los resultados obtenidos a partir de la aplicación de un proceso de entrenamiento de modelos de aprendizaje máquina en grandes volúmenes de datos, mediante el uso de herramientas de visualización como gráficas de dispersión, de tendencia central, mapas de calor y curvas ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd792de0-251b-49ad-8da2-6e400013c66a",
   "metadata": {},
   "source": [
    "#### **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815ab9f0-6b99-478d-8037-03433f3584f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, sum, when, split, col, lit, max, min, expr\n",
    "from pyspark.sql.functions import to_date, var_samp, variance, var_pop, month, to_timestamp, dayofweek\n",
    "from pyspark.sql.functions import hour, month\n",
    "from pyspark.sql.types import NumericType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, round, concat_ws, desc, when, concat\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Imports adicionales necesarios\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when, count as spark_count\n",
    "import numpy as np\n",
    "\n",
    "# Visualización avanzada - siguiendo metodologías de la Sesión 5\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import altair as alt\n",
    "\n",
    "# Configurar estilos de visualización\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a54271-e1dc-4cc9-8c31-814bc73a46ba",
   "metadata": {},
   "source": [
    "### **1. Definir un proceso de validación cruzada**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42191c-b31e-4dd4-ad7a-20070b5c2026",
   "metadata": {},
   "source": [
    "Para ello y partiendo de la muestra M = {Mi: Mi es una partición derivada de las variables de caracterización de la población} construida en el paso 1 de la actividad 4 del Módulo 5, se debe de determinar un valor “k” para el proceso de validación cruzada “k-fold” a implementar. Este valor deberá de ser argumentado con profundidad, de tal forma que se garantice que cada uno de los “k-fold” generados, sean muestras representativas de la población."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e9097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 22:35:35 WARN Utils: Your hostname, Paulinas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.241 instead (on interface en0)\n",
      "25/06/17 22:35:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/17 22:35:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Maestria_evidencia1\") \\\n",
    "    .config(\"spark.driver.memory\", \"64g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"16g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Mejores tablas\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") # Quitar warnings innecesarios de Jupyter\n",
    "#spark, comentando el comando del environment para reducir el ruido del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbbad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:================================================>     (130 + 12) / 146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de registros: 109950731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "####\n",
    "#### La base de datos ha sido limpiada y modificada a este punto\n",
    "#### \n",
    "####\n",
    "file_path = \"/Users/pauescalante/Documents/Maestria/Trimestre 7/BigData/big-data-act/DataModified/expanded_database_ecommerce\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Mostrar cuantos registros se tienen inicialmete para en el futuro reducir a una dimensión contenida\n",
    "initial_total_count = df.count()\n",
    "print(f\"Número total de registros: {initial_total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbdae4c-4fd2-4bb3-b4d4-0c3173b6253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: date (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_time_ts: timestamp (nullable = true)\n",
      " |-- parent_category: string (nullable = true)\n",
      " |-- subcategory: string (nullable = true)\n",
      " |-- price_bucket: string (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imprimiendo el esquema del Dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef24d0a1-0b83-44af-96f3-034dd4aca558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------\n",
      " event_time      | 2019-11-17                           \n",
      " event_type      | view                                 \n",
      " product_id      | 5300440                              \n",
      " category_id     | 2053013563173241677                  \n",
      " brand           | vitek                                \n",
      " price           | 17.76                                \n",
      " user_id         | 513341639                            \n",
      " user_session    | d9544029-2739-4d16-9cac-79650460d9f0 \n",
      " event_time_ts   | 2019-11-17 05:35:32                  \n",
      " parent_category | None                                 \n",
      " subcategory     | None                                 \n",
      " price_bucket    | low                                  \n",
      " day_of_week     | 1                                    \n",
      " is_weekend      | true                                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el primer registro de ejemplo para visualizar las columnas\n",
    "df.show(n=1,truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a122615-1979-474d-a1d0-396fac970bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este punto en actividades previas se limpiaron los datos\n",
    "# Pero haremos una última limpieza para verificar\n",
    "\n",
    "#Se eliminan registros con valores nulos\n",
    "df_clean = df.dropna()\n",
    "\n",
    "#Se eliminan columnas con valores nulos\n",
    "df_clean = df_clean.na.drop()\n",
    "\n",
    "#Se eliminan registros duplicados\n",
    "df_clean = df_clean.dropDuplicates()\n",
    "df = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b93b42-35c9-490b-aba3-eed6f08f43f5",
   "metadata": {},
   "source": [
    "#### **Variables de categorízación definidas en actividades previas**\n",
    "\n",
    "EVENT_TYPE:\n",
    "   - Categorías: view, cart, purchase\n",
    "   - Distribución: Altamente desbalanceada (~98.5% no-purchase)\n",
    "   - Implicación: Clase minoritaria requiere cuidado especial\n",
    "\n",
    "PRICE_BUCKET:\n",
    "   - Categorías: low, medium, high  \n",
    "   - Distribución: Relativamente balanceada (~33% cada una)\n",
    "   - Implicación: Facilita estratificación\n",
    "\n",
    "COMBINACIÓN (STRATUM):\n",
    "   - Total estratos: 9 posibles (3 x 3)\n",
    "   - Distribución: Variable según intersección de categorías\n",
    "   - Implicación: Necesidad de suficientes muestras por estrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04adfc3-d98c-4ef8-9a29-f3b982f40d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:========================================>               (23 + 9) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra para análisis: 1,093 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Aplicar muestreo inicial para hacer el análisis manejable\n",
    "# Se han explorado SRS y stratified pero SRS ha sido suficiente para los datos dada su distribución de las variables categóricas\n",
    "sample_df = df.sample(fraction=0.00001, seed=42)  # 0.001% para análisis inicial\n",
    "print(f\"Muestra para análisis: {sample_df.count():,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ca2b8e-3dc5-4c03-907e-d6442b735fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:==========================================>             (24 + 8) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño final de muestra para validación cruzada: 1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Variables de caracterización: event_type y price_bucket\n",
    "sample_df_base = sample_df\n",
    "sample_df_with_stratum = sample_df_base.withColumn(\"stratum\", \n",
    "                                                   concat(sample_df_base[\"event_type\"], \n",
    "                                                         lit(\"_\"), \n",
    "                                                         sample_df_base[\"price_bucket\"]))\n",
    "\n",
    "# Muestreo estratificado\n",
    "target_sample_size = 5000\n",
    "total_count = sample_df_with_stratum.count()\n",
    "\n",
    "strata_counts = sample_df_with_stratum.groupBy(\"stratum\").count().collect()\n",
    "fractions = {}\n",
    "for row in strata_counts:\n",
    "    stratum = row[\"stratum\"]\n",
    "    count = row[\"count\"]\n",
    "    proportion = count / total_count\n",
    "    target_stratum_size = target_sample_size * proportion\n",
    "    sampling_fraction = target_stratum_size / count\n",
    "    fractions[stratum] = sampling_fraction if sampling_fraction <= 1.0 else 1.0\n",
    "c\n",
    "# Generar muestra estratificada final\n",
    "sample_df = sample_df_with_stratum.sampleBy(\"stratum\", fractions=fractions, seed=42)\n",
    "print(f\"Tamaño final de muestra para validación cruzada: {sample_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f740ef0-06c6-41bd-a5b2-993b16c5ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sample_size_visually(df, target_col=\"event_type\", batch_size=1000, max_batches=50):\n",
    "    \"\"\"\n",
    "    Determina visualmente el tamaño de muestra adecuado siguiendo la metodología\n",
    "    de la Sesión 5 - análisis iterativo por batches\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame de Spark\n",
    "        target_col: Columna objetivo para análisis\n",
    "        batch_size: Tamaño de cada batch\n",
    "        max_batches: Número máximo de batches a analizar\n",
    "    \n",
    "    Returns:\n",
    "        List de estadísticas por batch\n",
    "    \"\"\"\n",
    "    print(\"=== ANÁLISIS VISUAL DE TAMAÑO DE MUESTRA ===\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Analizando hasta {max_batches} batches\")\n",
    "    \n",
    "    # Añadir ID aleatorio para sampling\n",
    "    window_spec = Window.orderBy(rand())\n",
    "    df_with_id = df.withColumn(\"sample_id\", row_number().over(window_spec))\n",
    "    \n",
    "    # Preparar variables objetivo\n",
    "    df_analysis = df_with_id.withColumn(\"purchase_rate\", \n",
    "                                       when(col(target_col) == \"purchase\", 1.0).otherwise(0.0))\n",
    "    \n",
    "    # Arrays para almacenar estadísticas por batch\n",
    "    statistics_batch = []\n",
    "    \n",
    "    # Obtener el primer batch\n",
    "    current_batch = df_analysis.filter(f\"sample_id <= {batch_size}\")\n",
    "    \n",
    "    for batch_num in range(1, max_batches + 1):\n",
    "        end_id = batch_num * batch_size\n",
    "        current_batch = df_analysis.filter(f\"sample_id <= {end_id}\")\n",
    "        \n",
    "        # Calcular estadísticas del batch acumulativo\n",
    "        batch_count = current_batch.count()\n",
    "        if batch_count == 0:\n",
    "            break\n",
    "            \n",
    "        # Métricas clave para análisis de estabilidad\n",
    "        avg_price = current_batch.select(avg(\"price\")).collect()[0][0] or 0\n",
    "        purchase_rate = current_batch.select(avg(\"purchase_rate\")).collect()[0][0] or 0\n",
    "        std_price = current_batch.select(stddev(\"price\")).collect()[0][0] or 0\n",
    "        \n",
    "        statistics_batch.append({\n",
    "            'batch_number': batch_num,\n",
    "            'sample_size': batch_count,\n",
    "            'avg_price': avg_price,\n",
    "            'purchase_rate': purchase_rate,\n",
    "            'std_price': std_price\n",
    "        })\n",
    "        \n",
    "        print(f\"Batch {batch_num}: {batch_count:,} registros, \"\n",
    "              f\"Purchase rate: {purchase_rate:.4f}, \"\n",
    "              f\"Avg price: {avg_price:.2f}\")\n",
    "        \n",
    "        # Criterio de parada: estabilización de métricas! \n",
    "        # En este caso usamos purchase_rate ya que no tenemos muchas variables numéricas disponibles\n",
    "        if batch_num >= 10:\n",
    "            recent_rates = [s['purchase_rate'] for s in statistics_batch[-5:]]\n",
    "            cv_rate = np.std(recent_rates) / np.mean(recent_rates) if np.mean(recent_rates) > 0 else float('inf')\n",
    "            \n",
    "            if cv_rate < 0.05:  # CV < 5% indica estabilización\n",
    "                print(f\"Estabilización detectada en batch {batch_num} (CV: {cv_rate:.4f})\")\n",
    "                break\n",
    "    \n",
    "    return statistics_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c6718-b4b5-4ff0-b600-f418b5e4e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISIS VISUAL DE TAMAÑO DE MUESTRA ===\n",
      "Batch size: 500\n",
      "Analizando hasta 30 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:===============================>                      (84 + 12) / 146]"
     ]
    }
   ],
   "source": [
    "# Realizar análisis visual de tamaño de muestra\n",
    "batch_statistics = analyze_sample_size_visually(sample_df, batch_size=500, max_batches=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45fd63-ac96-4706-98a9-221d89dd90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir estadísticas a DataFrame para visualización\n",
    "stats_df = pd.DataFrame(batch_statistics)\n",
    "\n",
    "# Crear visualización\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Análisis de Estabilidad de Muestra por Batches', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Purchase Rate por tamaño de muestra\n",
    "axes[0, 0].plot(stats_df['sample_size'], stats_df['purchase_rate'], \n",
    "                marker='o', linewidth=2, markersize=6, color='blue')\n",
    "axes[0, 0].set_title('Estabilización de Purchase Rate')\n",
    "axes[0, 0].set_xlabel('Tamaño de Muestra')\n",
    "axes[0, 0].set_ylabel('Purchase Rate')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Añadir línea de estabilización\n",
    "if len(stats_df) > 5:\n",
    "    stable_rate = stats_df['purchase_rate'].tail(5).mean()\n",
    "    axes[0, 0].axhline(y=stable_rate, color='red', linestyle='--', \n",
    "                      label=f'Rate Estable: {stable_rate:.4f}')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "# 2. Precio promedio por tamaño de muestra\n",
    "axes[0, 1].plot(stats_df['sample_size'], stats_df['avg_price'], \n",
    "                marker='s', linewidth=2, markersize=6, color='green')\n",
    "axes[0, 1].set_title('Estabilización de Precio Promedio')\n",
    "axes[0, 1].set_xlabel('Tamaño de Muestra')\n",
    "axes[0, 1].set_ylabel('Precio Promedio')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Desviación estándar del precio\n",
    "axes[1, 0].plot(stats_df['sample_size'], stats_df['std_price'], \n",
    "                marker='^', linewidth=2, markersize=6, color='orange')\n",
    "axes[1, 0].set_title('Estabilización de Desviación Estándar')\n",
    "axes[1, 0].set_xlabel('Tamaño de Muestra')\n",
    "axes[1, 0].set_ylabel('Std Dev Precio')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Coeficiente de variación (indicador de estabilidad)\n",
    "cv_values = []\n",
    "for i in range(5, len(stats_df)):\n",
    "    recent_rates = stats_df['purchase_rate'].iloc[i-4:i+1]\n",
    "    cv = recent_rates.std() / recent_rates.mean() if recent_rates.mean() > 0 else 0\n",
    "    cv_values.append(cv)\n",
    "\n",
    "if cv_values:\n",
    "    axes[1, 1].plot(stats_df['sample_size'].iloc[5:], cv_values, \n",
    "                    marker='d', linewidth=2, markersize=6, color='red')\n",
    "    axes[1, 1].axhline(y=0.05, color='black', linestyle='--', \n",
    "                      label='Umbral Estabilidad (5%)')\n",
    "    axes[1, 1].set_title('Coeficiente de Variación (Ventana 5 batches)')\n",
    "    axes[1, 1].set_xlabel('Tamaño de Muestra')\n",
    "    axes[1, 1].set_ylabel('CV Purchase Rate')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f7fe0-0ba9-47b5-86cb-887a1ec37e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar tamaño de muestra óptimo y valor K\n",
    "optimal_sample_size = stats_df['sample_size'].iloc[-1]\n",
    "print(f\"Tamaño de muestra estabilizado: {optimal_sample_size:,} registros\")\n",
    "\n",
    "# Calcular K basado en criterios de representatividad y eficiencia computacional\n",
    "# Parece ser que 5000 muestras tiene sentido ya que llega a la media \n",
    "min_samples_per_fold = 1000  # Mínimo para mantener representatividad\n",
    "max_k = optimal_sample_size // min_samples_per_fold\n",
    "recommended_k = __builtins__.min(5, max_k)  # Máximo 5 para eficiencia computacional\n",
    "\n",
    "print(f\"Máximo K posible: {max_k}\")\n",
    "print(f\"K recomendado: {recommended_k}\")\n",
    "print(f\"Registros por fold: {optimal_sample_size // recommended_k:,}\")\n",
    "\n",
    "K_FOLDS = recommended_k\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9e6c-96ba-490c-a8ef-77e809599c32",
   "metadata": {},
   "source": [
    "### **2.  Construcción de los “k-folds”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86686301-b0b6-4a13-b383-7f6c06ffbc23",
   "metadata": {},
   "source": [
    "Para poblar cada uno de los “k-fold”, tomar en cuenta el proceso de muestreo que se propuso desde la Actividad 3 del Módulo 4. Tomar en cuenta que eventualmente el volumen de datos es alto (Big Data), por lo que, entre más pliegues generados, la experimentación será más costosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb99c9-8235-440b-ad39-c155df98d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para validar (y peace of mind) se analiza la distrubución de los valores iniciales\n",
    "# En este caso podemos observar que mantiene la distribución del valor objetivo!\n",
    "df_with_label = df.withColumn(\"label\", \n",
    "                             when(col(\"event_type\") == \"purchase\", 1.0)\n",
    "                             .otherwise(0.0))\n",
    "\n",
    "# Calculate original distribution\n",
    "print(\"=== COMPARACIÓN DE DISTRIBUCIONES POBLACION ORIGINAL===\")\n",
    "print(f\"Dataset original: {df.count():,} registros\")\n",
    "\n",
    "original_class_dist = df_with_label.groupBy(\"label\").count().collect()\n",
    "original_total = df.count()\n",
    "\n",
    "print(\"\\nDistribución de clases en dataset original:\")\n",
    "for row in original_class_dist:\n",
    "    label = \"Compra\" if row[\"label\"] == 1.0 else \"No Compra\"\n",
    "    count = row[\"count\"]\n",
    "    percentage = (count / original_total) * 100\n",
    "    print(f\"{label}: {count:,} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53186b86-d5ca-4cdf-a128-3f101b455c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar el tamaño de muestra determinado visualmente\n",
    "final_sample_fraction = optimal_sample_size / df.count()\n",
    "supervised_df = sample_df.sample(fraction=final_sample_fraction, seed=RANDOM_SEED)\n",
    "\n",
    "# Crear variable objetivo binaria\n",
    "supervised_df = supervised_df.withColumn(\"label\", \n",
    "                                       when(col(\"event_type\") == \"purchase\", 1.0)\n",
    "                                       .otherwise(0.0))\n",
    "\n",
    "print(f\"Muestra final para cross-validation: {supervised_df.count():,} registros\")\n",
    "\n",
    "# Verificar distribución de clases\n",
    "class_dist = supervised_df.groupBy(\"label\").count().collect()\n",
    "total_sample = supervised_df.count()\n",
    "\n",
    "print(\"\\nDistribución de clases en la muestra final:\")\n",
    "for row in class_dist:\n",
    "    label = \"Compra\" if row[\"label\"] == 1.0 else \"No Compra\"\n",
    "    count = row[\"count\"]\n",
    "    percentage = (count / total_sample) * 100\n",
    "    print(f\"{label}: {count:,} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef05caa-976c-406b-89bb-c9f85d61fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_folds(df, k_folds=5, stratify_col=\"stratum\", seed=42):\n",
    "    \"\"\"\n",
    "    Crea K-folds estratificados para PySpark DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Creando {k_folds} folds estratificados...\")\n",
    "    \n",
    "    # Crear ventana para asignar números de fila por estrato\n",
    "    window_spec = Window.partitionBy(stratify_col).orderBy(rand(seed))\n",
    "    \n",
    "    # Asignar número de fila dentro de cada estrato\n",
    "    df_with_row_num = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "    \n",
    "    # Crear expresión SQL para asignar fold_id\n",
    "    fold_assignment_expr = f\"(row_num - 1) % {k_folds}\"\n",
    "    \n",
    "    df_with_folds = df_with_row_num.withColumn(\"fold_id\", \n",
    "                                               expr(fold_assignment_expr).cast(\"int\"))\n",
    "    \n",
    "    # Verificar distribución\n",
    "    fold_distribution = df_with_folds.groupBy(\"fold_id\", stratify_col).count().orderBy(\"fold_id\", stratify_col)\n",
    "    print(\"\\nDistribución de estratos por fold:\")\n",
    "    fold_distribution.show(50)\n",
    "    \n",
    "    return df_with_folds.drop(\"row_num\")\n",
    "\n",
    "# Aplicar stratified k-fold\n",
    "df_with_folds = create_stratified_folds(supervised_df, k_folds=K_FOLDS, seed=42)\n",
    "\n",
    "print(f\"DataFrame con folds creado. Total de registros: {df_with_folds.count()}\")\n",
    "\n",
    "# Verificar calidad de estratificación\n",
    "# Contar registros por fold\n",
    "fold_counts = df_with_folds.groupBy(\"fold_id\").count().orderBy(\"fold_id\")\n",
    "print(\"Tamaño de cada fold:\")\n",
    "fold_counts.show()\n",
    "\n",
    "# Verificar distribución de clases por fold\n",
    "class_distribution = df_with_folds.groupBy(\"fold_id\", \"label\").count().orderBy(\"fold_id\", \"label\")\n",
    "print(\"Distribución de clases por fold:\")\n",
    "class_distribution.show()\n",
    "\n",
    "# Calcular proporción de clase positiva por fold\n",
    "positive_ratios = []\n",
    "for fold_id in range(K_FOLDS):\n",
    "    fold_data = df_with_folds.filter(col(\"fold_id\") == fold_id)\n",
    "    total = fold_data.count()\n",
    "    positive = fold_data.filter(col(\"label\") == 1.0).count()\n",
    "    ratio = positive / total * 100\n",
    "    positive_ratios.append(ratio)\n",
    "    print(f\"Fold {fold_id}: {positive}/{total} ({ratio:.2f}% positivos)\")\n",
    "\n",
    "print(f\"\\nVariabilidad en proporción de positivos:\")\n",
    "print(f\"Media: {np.mean(positive_ratios):.2f}%\")\n",
    "print(f\"Desviación estándar: {np.std(positive_ratios):.2f}%\")\n",
    "print(f\"Rango: {np.min(positive_ratios):.2f}% - {np.max(positive_ratios):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75a495-49d0-4bfc-9913-d2bb9c56617c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **3.Experimentación**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90681b-23a0-4995-8169-bc383e5d8617",
   "metadata": {},
   "source": [
    "Para esta etapa y generando los conjuntos de entrenamiento y prueba que se pueden construir a partir de los “k-folds” generados en la etapa previa, realizar el proceso de entrenamiento a partir del algoritmo que mejor reportó resultados en la Actividad 4 del Módulo 5. Realizar la etapa de entrenamiento y registrar los resultados necesarios y suficientes para obtener el modelo que mejor generaliza los patrones que se aprenden de cada experimento realizado. La medición de calidad de resultado será a partir de las métricas que se establecieron como las mejores identificadas en la actividad 4 del Módulo 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013731f-8438-4498-ba3d-a4b1e44a6174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=== GENERANDO REPORTE AUTOMÁTICO DE EDA ===\")\n",
    "\n",
    "# Desactivar Arrow temporalmente por memory limit\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "\n",
    "try:\n",
    "    eda_sample = supervised_df.sample(fraction=0.1, seed=42).toPandas()\n",
    "    \n",
    "    profile = ProfileReport(eda_sample, \n",
    "                          title=\"EDA E-commerce Purchase Prediction\",\n",
    "                          explorative=True)\n",
    "    \n",
    "    profile.to_notebook_iframe()\n",
    "finally:\n",
    "    # Reactivar Arrow\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d9427-67c1-41c2-8f6a-fec62d55326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear metodos para el ml pipeline\n",
    "def create_ml_pipeline():\n",
    "    \"\"\"\n",
    "    Crea el pipeline de ML basado en la Actividad 4\n",
    "    \"\"\"\n",
    "    # Variables numéricas y categóricas\n",
    "    feature_cols = [\"price\", \"day_of_week\"]\n",
    "    categorical_cols = [\"brand\", \"parent_category\", \"price_bucket\"]\n",
    "    \n",
    "    # Indexadores para variables categóricas\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"skip\") \n",
    "                for col in categorical_cols]\n",
    "    \n",
    "    # Ensamblador de características\n",
    "    feature_cols_final = feature_cols + [f\"{col}_indexed\" for col in categorical_cols]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols_final, outputCol=\"features\")\n",
    "    \n",
    "    # Escalador\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    \n",
    "    # Modelo con hiperparámetros óptimos de Actividad 4\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"scaledFeatures\", \n",
    "        labelCol=\"label\",\n",
    "        regParam=0.01,\n",
    "        elasticNetParam=0.0,\n",
    "        maxIter=10\n",
    "    )\n",
    "    \n",
    "    # Pipeline completo\n",
    "    pipeline = Pipeline(stages=indexers + [assembler, scaler, lr])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate_model(predictions):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo usando las métricas de la Actividad 4\n",
    "    \"\"\"\n",
    "    # Evaluadores\n",
    "    auc_roc_evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        rawPredictionCol=\"rawPrediction\", \n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    auc_prc_evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        rawPredictionCol=\"rawPrediction\", \n",
    "        metricName=\"areaUnderPR\"\n",
    "    )\n",
    "    \n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    # Calcular métricas\n",
    "    auc_roc = auc_roc_evaluator.evaluate(predictions)\n",
    "    auc_prc = auc_prc_evaluator.evaluate(predictions)\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    \n",
    "    return {\n",
    "        'auc_roc': auc_roc,\n",
    "        'auc_prc': auc_prc,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef00594-fff2-41a1-8dc9-7d3c4bd2680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = []\n",
    "trained_models = []\n",
    "\n",
    "# KFOLDS fue definido anteriormente\n",
    "for fold in range(K_FOLDS):\n",
    "    print(f\"\\n--- FOLD {fold + 1}/{K_FOLDS} ---\")\n",
    "    \n",
    "    # Dividir datos\n",
    "    test_data = df_with_folds.filter(col(\"fold_id\") == fold)\n",
    "    train_data = df_with_folds.filter(col(\"fold_id\") != fold)\n",
    "    \n",
    "    train_count = train_data.count()\n",
    "    test_count = test_data.count()\n",
    "    \n",
    "    print(f\"Train: {train_count} muestras\")\n",
    "    print(f\"Test: {test_count} muestras\")\n",
    "    \n",
    "    # Crear y entrenar pipeline\n",
    "    pipeline = create_ml_pipeline()\n",
    "    \n",
    "    print(\"Entrenando modelo...\")\n",
    "    model = pipeline.fit(train_data)\n",
    "    trained_models.append(model)\n",
    "    \n",
    "    # Generar predicciones\n",
    "    print(\"Generando predicciones...\")\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    metrics = evaluate_model(predictions)\n",
    "    \n",
    "    # Agregar información del fold\n",
    "    fold_result = {\n",
    "        'fold': fold,\n",
    "        'train_size': train_count,\n",
    "        'test_size': test_count,\n",
    "        **metrics\n",
    "    }\n",
    "    cv_results.append(fold_result)\n",
    "    \n",
    "    print(f\"Resultados Fold {fold + 1}:\")\n",
    "    print(f\"  AUC-ROC: {metrics['auc_roc']:.4f}\")\n",
    "    print(f\"  AUC-PRC: {metrics['auc_prc']:.4f}\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb599b41-cdd5-48bf-bd67-89894552b36d",
   "metadata": {},
   "source": [
    "### **4. Resultados**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ca737-109f-4b9a-a4ee-32ec78a4500e",
   "metadata": {},
   "source": [
    " A partir de la elección de diferentes bibliotecas para la visualización de resultados, se deberán de mostrar gráficas que te permitan visualizar los resultados de la etapa de entrenamiento del paso previo (resultados de cada Fold a partir de las métricas usadas para medir dichos resultados, evolución de las etapas de entrenamiento para detectar sobre-ajuste, estadísticas generales para medir la variabilidad de los resultados obtenidos, entre otras que juzgues pertinentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36016bcb-f68c-409b-ba96-625c1156ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para visualizaciones\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# 1. Gráfica de barras con resultados por fold\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Resultados de Validación Cruzada K-Fold (K=5)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# AUC-ROC por fold\n",
    "axes[0, 0].bar(results_df['fold'] + 1, results_df['auc_roc'], \n",
    "               color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes[0, 0].axhline(y=results_df['auc_roc'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Media: {results_df[\"auc_roc\"].mean():.4f}')\n",
    "axes[0, 0].set_title('AUC-ROC por Fold')\n",
    "axes[0, 0].set_xlabel('Fold')\n",
    "axes[0, 0].set_ylabel('AUC-ROC')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC-PRC por fold\n",
    "axes[0, 1].bar(results_df['fold'] + 1, results_df['auc_prc'], \n",
    "               color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes[0, 1].axhline(y=results_df['auc_prc'].mean(), color='blue', linestyle='--', \n",
    "                   label=f'Media: {results_df[\"auc_prc\"].mean():.4f}')\n",
    "axes[0, 1].set_title('AUC-PRC por Fold')\n",
    "axes[0, 1].set_xlabel('Fold')\n",
    "axes[0, 1].set_ylabel('AUC-PRC')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy por fold\n",
    "axes[1, 0].bar(results_df['fold'] + 1, results_df['accuracy'], \n",
    "               color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "axes[1, 0].axhline(y=results_df['accuracy'].mean(), color='purple', linestyle='--', \n",
    "                   label=f'Media: {results_df[\"accuracy\"].mean():.4f}')\n",
    "axes[1, 0].set_title('Accuracy por Fold')\n",
    "axes[1, 0].set_xlabel('Fold')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparación de métricas normalizadas\n",
    "fold_labels = [f'Fold {i+1}' for i in range(K_FOLDS)]\n",
    "x = np.arange(len(fold_labels))\n",
    "width = 0.25\n",
    "\n",
    "# Normalizar métricas\n",
    "auc_roc_norm = (results_df['auc_roc'] - results_df['auc_roc'].min()) / (results_df['auc_roc'].max() - results_df['auc_roc'].min())\n",
    "auc_prc_norm = (results_df['auc_prc'] - results_df['auc_prc'].min()) / (results_df['auc_prc'].max() - results_df['auc_prc'].min())\n",
    "accuracy_norm = (results_df['accuracy'] - results_df['accuracy'].min()) / (results_df['accuracy'].max() - results_df['accuracy'].min())\n",
    "\n",
    "axes[1, 1].bar(x - width, auc_roc_norm, width, label='AUC-ROC (norm)', alpha=0.7)\n",
    "axes[1, 1].bar(x, auc_prc_norm, width, label='AUC-PRC (norm)', alpha=0.7)\n",
    "axes[1, 1].bar(x + width, accuracy_norm, width, label='Accuracy (norm)', alpha=0.7)\n",
    "axes[1, 1].set_title('Comparación de Métricas Normalizadas')\n",
    "axes[1, 1].set_xlabel('Fold')\n",
    "axes[1, 1].set_ylabel('Valor Normalizado (0-1)')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(fold_labels)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e996693-1e7a-42b6-8b1f-129f42f5ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Box plots para análisis de variabilidad\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Distribución de Métricas en Validación Cruzada', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_data = [results_df['auc_roc'], results_df['auc_prc'], results_df['accuracy']]\n",
    "metric_names = ['AUC-ROC', 'AUC-PRC', 'Accuracy']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "for i, (data, name, color) in enumerate(zip(metrics_data, metric_names, colors)):\n",
    "    bp = axes[i].boxplot(data, patch_artist=True, notch=True)\n",
    "    bp['boxes'][0].set_facecolor(color)\n",
    "    bp['boxes'][0].set_alpha(0.7)\n",
    "    \n",
    "    # Agregar puntos individuales\n",
    "    y = data\n",
    "    x = np.random.normal(1, 0.04, size=len(y))\n",
    "    axes[i].scatter(x, y, alpha=0.6, color='darkblue', s=50)\n",
    "    \n",
    "    axes[i].set_title(f'{name}\\nMedia: {data.mean():.4f} ± {data.std():.4f}')\n",
    "    axes[i].set_ylabel('Valor de Métrica')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Estadísticas\n",
    "    stats_text = f'CV: {(data.std()/data.mean()*100):.2f}%\\nRango: {data.max()-data.min():.4f}'\n",
    "    axes[i].text(0.02, 0.98, stats_text, transform=axes[i].transAxes, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7abec2-0119-43a3-bd47-7af62f663acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Análisis de correlación y tendencias\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Análisis de Estabilidad y Correlaciones', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Evolución de métricas por fold\n",
    "folds = results_df['fold'] + 1\n",
    "axes[0, 0].plot(folds, results_df['auc_roc'], 'o-', label='AUC-ROC', linewidth=2, markersize=8)\n",
    "axes[0, 0].plot(folds, results_df['auc_prc'], 's-', label='AUC-PRC', linewidth=2, markersize=8)\n",
    "axes[0, 0].plot(folds, results_df['accuracy'], '^-', label='Accuracy', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_title('Evolución de Métricas por Fold')\n",
    "axes[0, 0].set_xlabel('Fold')\n",
    "axes[0, 0].set_ylabel('Valor de Métrica')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Matriz de correlación\n",
    "correlation_matrix = results_df[metrics_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[0, 1], cbar_kws={'label': 'Correlación'})\n",
    "axes[0, 1].set_title('Correlación entre Métricas')\n",
    "\n",
    "# Coeficiente de variación\n",
    "cv_values = [(results_df[metric].std() / results_df[metric].mean() * 100) for metric in metrics_cols]\n",
    "bars = axes[1, 0].bar(metric_names, cv_values, color=['skyblue', 'lightcoral', 'lightgreen'], alpha=0.7)\n",
    "axes[1, 0].set_title('Coeficiente de Variación por Métrica')\n",
    "axes[1, 0].set_ylabel('CV (%)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, cv in zip(bars, cv_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{cv:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Mejora sobre baseline random\n",
    "baseline_auc = 0.5\n",
    "improvement = (results_df['auc_roc'] - baseline_auc) / baseline_auc * 100\n",
    "\n",
    "axes[1, 1].bar(folds, improvement, color='green', alpha=0.7)\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='-', linewidth=2, label='Baseline Random')\n",
    "axes[1, 1].set_title('Mejora sobre Baseline Random (%)')\n",
    "axes[1, 1].set_xlabel('Fold')\n",
    "axes[1, 1].set_ylabel('Mejora (%)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386fbb0-7357-4876-9bc6-734e1891604c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff0c891-8a54-49aa-8751-02b3d8207809",
   "metadata": {},
   "source": [
    "### **4. Discusión y conclusiones**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5034b5-6dcb-4eda-8d72-7462b89e5a55",
   "metadata": {},
   "source": [
    "En esta última sección, deberás de realizar un análisis de los resultados obtenidos de acuerdo a la tarea de aprendizaje que te has planteado en tu proyecto, de tal forma que se analice que tan significativos son los resultados obtenidos, la variabilidad que se obtuvo en los experimentos, para poder determinar qué tan significativos son estos resultados de acuerdo a la tarea de aprendizaje plantada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45495abc-2a10-4c91-a33e-8ea5c7af9de0",
   "metadata": {},
   "source": [
    "La validación cruzada K-fold es una excelente herramienta que resalta insights curiosos de la naturaleza de los datos de e-commerce. Mientras que nuestro modelo de regresión logística muestra una estabilidad impresionante con variaciones menores al 5% entre experimentos, al mismo tiempo nos enseña algo importante sobre la diferencia entre ser técnicamente correcto y ser prácticamente útil.\n",
    "\n",
    "Los resultados, con un AUC-ROC promedio de 0.68 y un AUC-PRC de apenas 0.04, nos hacen una pregunta profunda: ¿puede un modelo ser \"exitoso\" cuando predice consistentemente algo que casi nunca pasa? Cuando solo el 2% de las personas que navegan en una tienda online realmente compran algo, nos enfrentamos a un reto muy interesante. En el mundo de Big Data, podemos ser muy buenos midiendo algo que tal vez no estamos entendiendo completamente, lo que cuestiona nuestras ideas tradicionales sobre qué significa tener un modelo \"exitoso\" cuando estudiamos cómo se comportan las personas en línea. En si puede ser que los datos que recolectamos no sean muy útiles para lo que queremos medir o predecir de personas.\n",
    "\n",
    "Lo más interesante es que la baja variabilidad entre experimentos (menos del 5% de diferencia) puede señalar que hemos logrado capturar de manera muy consistente una señal muy débil, lo que sugiere que el problema no está en nuestro modelo sino en algo más profundo sobre cómo funciona la decisión de compra. \n",
    "\n",
    "**Este descubrimiento nos invita a pensar diferente sobre el machine learning aplicado al comportamiento humano: tal vez la \"falla\" de nuestro modelo no es un error técnico sino un hint sobre la realidad que la decisión de comprar algo surge de una complejidad humana que va más allá de las variables que podemos medir.** La consistencia de nuestros resultados se convierte entonces en una ventana hacia una verdad más grande sobre cómo se comportan los consumidores digitales: quizás la incertidumbre no es algo que debemos \"arreglar\" sino una característica natural del sistema que estamos estudiando, y aquí es donde una empresa puede llegar a cambiar su enfoque en esas areas ya que la mayoría de sus clientes no compra productos si no solo navega y absorbe información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723dc80-0f2b-4ece-b6b1-c05a9471c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los modelos\n",
    "base_path = \"/Users/pauescalante/Documents/Maestria/Trimestre 7/BigData/big-data-act/Models/\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "lr_model_path = f\"{base_path}/logistic_regression_model_act5_{timestamp}\"\n",
    "try:\n",
    "    lr_model.write().overwrite().save(lr_model_path)\n",
    "    print(f\"Modelo LR guardado en: {lr_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error guardando LR: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
